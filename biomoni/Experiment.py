import numpy as np
import pandas as pd
import glob
import os.path
import warnings
from copy import deepcopy

#ELELE

class Experiment:
    """
    A class for creating an Experiment object with preprocessed datasets e.g. offline(HPLC, bio dry mass), online (MFCS), CO2(Bluesense) data. The class assumes that there is a common path (path) where the whole data and the metadata is present within.
    It is assumed that files of the same types e.g. offline are called the same in every subfolder: e.g. the offline files in F1 and F2 (two experiment days - two subfolders in path) are named "offline.csv" in both subfolders. 
    It is also asumed that the raw data from types like "offline.csv" is generated by the same device and is thus idenical and requires identical settings to read.
    It is also assumed that the Experiment names (sub folder names for experiments) are equal to the experiment names in the metadata index column in order to get the right settings.
    If that is not the case and the folder with the measurement data is named differently as the id in metadata or if the directory is not present within path, consider to use exp_dir_manual to give the directory of the measurement data.

    :param path: Path of overarching directory, the metadata.xlsx file and the directories containing the data (e.g. online, offline and CO2) must be within this directory.
    :type path: str
    :param exp_id: Identifier of the right experiment, must match with index column in metadata.
    :type exp_id: int or str
    :param meta_path: metadata file name within path.
    :type meta_path: str
    :param types: Dict with measurement data types (desired names in Experiment object) as keys and file locations within path/exp_id (or exp_dir_manual) as value, e.g. {'CO2' : 'CO2_file.csv'}.
    :type types: dict
    :param exp_dir_manual: Manually given experiment path, can be used if the experiment data is in another directory than in the 'path' location where the metadata is, or if the data is in the same directory but the the experiment identifier (exp_id (e.g. F1) which is also the index column in metadata) does not match with the folder name, e.g. when folder name is created automatically and thus it`s hard to give the right exp_id for the Folder (eg. when the Folder is named "Exp_dateX_timeX" and is created automatically).
    :type exp_dir_manual: None or str
    :param index_ts: Dict with measurement data types as keys and index of timestamp column in raw data as value.
    :type index_ts: None or dict with ints
    :param read_csv_settings: Dict with measurement data types as keys and pd.read_csv settings to read the correspondig raw data to a pd.DataFrame as value.
    :type read_csv_settings: None or dict with settings
    :param to_datetime_settings: Dict with measurement data types as keys and settings to convert the timestamp column in raw data from string to pd.Timestamp as value.
    :type to_datetime_setings: None or dict with settings
    :param calc_rate: The first derivative is formed of the column 'X' in the dataframe of 'type'. E.g. {E.g 'on' : 'BASET'} calculates the 1. derivative of BASET (BASET_rate) in the 'on' data type.
    :type calc_rate: None or dict
    :param endpoint: Name of endpoint column in metadata. Helpful if you have several endpoints in your metadata.
    :type endpoint: str
    :param read_excel_settings: Settings to metadata excel file.
    :type read_excel_settings: None dict.
    :param smooth: dict with keys corresponding the measuring types (e.g. "on" , "CO2") and value corresponding to the columns to be smoothed e.g. [BASET, ACID].
    :type smooth: dict
    :param kwargs_smooth: kwargs to be used to smooth in 'pandas.DataFrame.ewm'
    :type kwargs_smooth: dict


    :return: Experiment object contaning the measurement data.

        """   


    def __repr__(self):
        """Representation of Experiment object in the print statement"""
        return  """Experiment(\"{path}\" , \"{exp_id}\")""".format(path= self.path, exp_id= self.exp_id)

    #format_ts kommt raus wenn du eh to_datetime_settings_gibst

    def __init__(self, path, exp_id, meta_path, types
    , exp_dir_manual = None
    , index_ts = None
    , read_csv_settings = None
    , to_datetime_settings = None
    , calc_rate = None
    , endpoint = "end1"
    , read_excel_settings = None
    , smooth = None
    , kwargs_smooth = dict(halflife=5, adjust= False)   #Martins smoothing settings, you may need to adjust this.

    ):
       

    # , filtering_columns = False, filter_on = ["base_rate"], filter_off = ["cX", "cS", "cE"], filter_CO2 = ["CO2"]
        pd.options.mode.chained_assignment = None       #because of pandas anoying warning message
        
        assert type(path) is str, "The given Path must be of type str"
        assert type(meta_path) is str, "The given meta_path must be of type str"
        assert all(isinstance(i, str) for i in types.values() ), "given file names must be strings"
       
        
        if index_ts is None:        #if no index_col is given, the timestamp column is assumed to be the first column (0) in the datafile
            index_ts = dict.fromkeys(types) #same keys as measurement types
            for typ in types.keys():
                index_ts[typ] = 0
        else: 
            assert index_ts.keys() == types.keys(), "The given type names must match with the keys of index_ts, read_csv_settings and to_datetime_settings"

        if read_csv_settings is None:
            read_csv_settings = dict.fromkeys(types)
            for typ in types.keys():
                read_csv_settings[typ] = {} #if no read csv setting are given **{} will result into nothin additional lateron in read
        else: 
            assert read_csv_settings.keys() == types.keys(), "The given type names must match with the keys of index_ts, read_csv_settings and to_datetime_settings"

        if to_datetime_settings is None:
            to_datetime_settings = dict.fromkeys(types)
            for typ in types.keys():
                to_datetime_settings[typ] = {}  #if no datetim settings are given **{} 
        else: 
            assert to_datetime_settings.keys() == types.keys(), "The given type names must match with the keys of index_ts, read_csv_settings and to_datetime_settings"

        
        if read_excel_settings is None:
            read_excel_settings = {}


        self.path = path 
        self.exp_id = exp_id
        self.endpoint = endpoint



        if exp_dir_manual is None:
            dir = os.path.join(path, exp_id)    #if no manual experiment name(exp_dir_manual - folder name) is given, the exp_id is assumed to be named as the subfolder within path.
            assert os.path.isdir(dir), "The directory {0} does not exist. If the experiment folder has another name as the identifier exp_id or if the experiment folder is not within path at all, consider to use exp_dir_manual as path name for the experiment data.".format(dir)
        else:
            dir = exp_dir_manual
            assert os.path.isdir(dir), "The directory {0} does not exist.".format(dir)

        file_path = {}
        for typ, filename in types.items():
            file_path[typ] = os.path.join(dir, filename)
           

        

        self.dataset = {}
        for typ, p in file_path.items():
            if os.path.isfile(p):   #checking if the file in the experiment folder even exists           
                self.dataset[typ] = self.read_data(path = p, index_ts = index_ts[typ], read_csv_settings = read_csv_settings[typ], to_datetime_settings = to_datetime_settings[typ])
            else:
                warnings.warn("The file {0} could not be found within the Experiment folder: {1}".format(types[typ], dir))
        
        if not self.dataset: raise TypeError("No data were collected, check if measurement data is in the right directory and if it is named correctly in the 'types' argument ")

        metadata_all = pd.read_excel(os.path.join(path, meta_path), index_col = 0, **read_excel_settings)
        metadata_all = metadata_all.astype(object).where(metadata_all.notnull(), None)  #load metadata and replace NaN and NaT values with None required for adequate time filtering in time_filter
        assert exp_id in metadata_all.index.values, "Experiment have to be in metadata"
        self.metadata = metadata_all.loc[exp_id]

        
        assert endpoint in metadata_all.columns, "Given endpoint must be in metadata columns"
        start = self.metadata["start"]
        end = self.metadata[endpoint]    

        for dskey in self.dataset.keys():   
            self.calc_t(dskey = dskey, start = start)

        self.dataset_raw = deepcopy(self.dataset)   #raw dataset, may be needed if changes are made on self.dataset after the Experiment object is created. For example if the feed_rate is required for the entire time span.

        for dskey in self.dataset.keys():
            self.time_filter(dskey, start, end)
        
        if calc_rate is not None:
            self.derive(calc_rate)
        
        if smooth is not None:
            self.ewm(smooth, kwargs_smooth)


    def read_data(self, path, index_ts, read_csv_settings, to_datetime_settings):
        """ Function to read the measurement data with the corresponding settings

        :param path: Path of the measurement data.
        :type path: str 
        :param index_ts: Index of the timestamp column
        :type index_ts: int
        :param read_csv_settings: Pandas read_csv setings for this type of data.
        :type read_csv_settings: dict
        :param to_datetime_settings: Pandas to_datetime settings to convert timestamp column to pd.Timestamp
        :type to_datetime_settings: dict
        """

        df = pd.read_csv(path, **read_csv_settings)
        df["ts"] = pd.to_datetime(df.iloc[:, index_ts], **to_datetime_settings)
        assert not df.empty, "The Dataframe is empty"
        return df    

    
    def time_filter(self, dskey, start = None, end = None):
        """Function to filter according to process time  

        :param dskey: Dataset key correspond to one of the measuring types (e.g. "off" , "CO2").
        :type dskey: dict key
        :param start: Timestamp of start point.
        :type start: pd.Timestamp or str
        :param end:  Timestamp of end point.
        :type end: pd.Timestamp or str
        :return: Filtered dataframes in dataset.

        """
        df = self.dataset[dskey]        # df = deepcopy(self.dataset[key] to avoid error message?) or pd.options.mode.chained_assignment = None in constructor

        start = pd.to_datetime(start)
        end = pd.to_datetime(end)

        #case distinction depening on given start and/or end time points
        if start is None:

            if end is None:
                pass
            
            elif end is not None:
                df = df[(df["ts"] <= end)]  

        elif start is not None:

            if end is None:
                df = df[(df["ts"] >= start)]  
            
            elif end is not None: 
                df = df[(df["ts"] >= start ) &  (df["ts"] <= end)] 

        assert not df.empty, "The Dataframe is empty"
        self.dataset[dskey] = df



    def calc_t(self, dskey, start = None):
        """ Calculates process time as decimal number.

        :param dskey: Dataset key correspond to one of the measuring types (e.g. "off" , "CO2")
        :type dskey: dict key
        :param start: Timestamp of start point
        :type start: pd.Timestamp or str
        :return: Time as decimal number in a new column "t"
        
        """

        df = self.dataset[dskey]

        if start is None:
            df["t"] = (df["ts"] - df["ts"][0]) / pd.Timedelta(1,"h")
        if start is not None:
            df["t"] = (df["ts"] - start) / pd.Timedelta(1,"h")

    
        df.set_index("t", inplace= True, drop= True)        #set t to index of dataframe
        
        self.dataset[dskey] = df



    def derive(self, calc_rate):
        """ Function to calculate the time derivative of a variable with finite differences.

        :param calc_rate_dict: dict with keys corresponding the measuring types (e.g. "on" , "CO2") and value corresponding to the columns to be derived e.g. [BASET, ACID].
        :type calc_rate_dict: dict
        :return: New column in dataframe named col_rate = time derivative of col.

        """

        assert type(calc_rate) is dict, "calc_rate needs to of type 'dict' not of type '{0}'".format(type(calc_rate)) 
        for typ, columns  in calc_rate.items():

            assert typ in self.dataset.keys(), "The given typ {0} must be in the data".format(typ)
            assert type(columns) in [str, int, list, tuple, set], "The type of the desired columns which you want to derive, should be a str an int or an iterable (list, tuple, set) with str's or int's not '{0}'".format(type(columns))
            df = self.dataset[typ]

            if type(columns) in (str, int):
                columns = [columns]
            for col in columns:
                assert type(col) in (str, int), "The column names must be of type str or int not of type {0}".format(type(col))


                if col in df.columns:
                    try:
                        df[col + "_rate"] = df[col].diff() / np.diff(df.index, prepend= 1)
                    
                    except:
                        df[col] = pd.to_numeric(df[col] , downcast="float" , errors="coerce") # some values in BASET were recognized as string
                        df[col + "_rate"] = df[col].diff() / np.diff(df.index, prepend= 1)
                else:
                    warnings.warn("The col '{0}' is not in the data '{1}'".format(col, typ))

                self.dataset[typ] = df

    
    def ewm(self, smooth, kwargs_smooth):
        """ Function to smooth the values of a column by exponentially weighted calculations with 'pandas.DataFrame.ewm'.

        :param smooth: dict with keys corresponding the measuring types (e.g. "on" , "CO2") and value corresponding to the columns to be smoothed e.g. [BASET, ACID].
        :type smooth: dict
        :param kwargs_smooth: kwargs to be used in 'pandas.DataFrame.ewm'
        :type kwargs_smooth: dict
        :return: New column in dataframe named col_smoothed = with smoothed values.

        """

        assert type(smooth) is dict, "smooth needs to of type 'dict' not of type '{0}'".format(type(smooth)) 
        for typ, columns  in smooth.items():

            assert typ in self.dataset.keys(), "The given typ {0} must be in the data".format(typ)
            assert type(columns) in [str, int, list, tuple, set], "The type of the desired columns which you want to smooth, should be a str an int or an iterable (list, tuple, set) with str's or int's not '{0}'".format(type(columns))
            df = self.dataset[typ]

            if type(columns) in (str, int):
                columns = [columns]
            for col in columns:
                assert type(col) in (str, int), "The column names must be of type str or int not of type {0}".format(type(col))

                if col in df.columns:
                    df[col+ "_smoothed"] = df[col].ewm(**kwargs_smooth).mean()
                else:
                    warnings.warn("The col '{0}' is not in the data '{1}'".format(col, typ))

                self.dataset[typ] = df
    
    def drop_col(self, col_info):
        """ Function to drop columns from pd.DataFrame of specific type on axis = 1.

        :param cols: dict with keys corresponding the measuring types (e.g. "on" , "CO2") and value corresponding to the columns to be dropped.
        :type cols: dict
        :return: New data with dropped columns.

        """
        assert type(col_info) is dict, "col_info needs to of type 'dict' not of type '{0}'".format(type(col_info)) 
        for typ, columns  in col_info.items():

            assert typ in self.dataset.keys(), "The given typ {0} must be in the data".format(typ)
            assert type(columns) in [str, int, list, tuple, set], "The type of the desired columns which you want to drop, should be a str an int or an iterable (list, tuple, set) with str's or int's not '{0}'".format(type(columns))
            df = self.dataset[typ]

            if type(columns) in (str, int):
                columns = [columns]
            for col in columns:
                assert type(col) in (str, int), "The column names must be of type str or int not of type {0}".format(type(col))
                
                if col in df.columns:
                    df.drop(col, axis = 1, inplace = True)
                else:
                    warnings.warn("The col '{0}' is not in the data '{1}'".format(col, typ))

                self.dataset[typ] = df



    def pop_dataframe(self, types):
        """Function to delete whole dataframes from the dataset, either of one or several types.

            :param types: Type of the measurement data.
            :type types: str or iterable with str`s (list, tuple, set).
            :return: Dataset without specific selected dataframe/s.
        """

        if type(types) in [list, tuple, set]:
            for typ in types:
                if typ in self.dataset.keys():
                    self.dataset.pop(typ)
                    
                else: 
                    warnings.warn("The type '{0}' must be in the dataset".format(typ))

        elif type(types) is str:
            if types in self.dataset.keys():
                self.dataset.pop(types)

            else:
                warnings.warn("The type '{0}' must be in the dataset".format(types))
        else:
            raise TypeError("Type of types must be a str or a list, tuple or np.ndarray with strings")


